# Article Generation Review (Accuracy, Engagement, SEO)

Date: 2026-01-12

This document describes how the article generation pipeline works end-to-end (enrichment → generation → citations → images → save/publish), how each “engine” behaves, and where the current design is likely to break or produce low-quality / risky outputs.

The focus is:
- Accuracy: reduce ungrounded claims and citation/link errors.
- Engagement: produce content users actually read.
- Search/Google: avoid patterns that get deindexed/ignored (thin/duplicate/templated content, weak E-E-A-T signals, broken references, poor metadata, etc.).

---

## 1) End-to-end pipeline map

### Primary entrypoints

- `src/generate.py`: CLI entrypoint. Loads most recent `data/enriched_*.json` and calls generation.
- `src/pipeline/orchestrator.py`:
  - `generate_articles_from_enriched(...)`: main sync orchestration.
  - `generate_articles_async(...)`: async/threaded variant intended for Python 3.14 free-threading.
  - `generate_single_article(...)`: turns a single `EnrichedItem` into a `GeneratedArticle`.
- `src/pipeline/file_io.py`: persists the article Markdown + frontmatter, and also runs citation resolution + image selection before writing.

### Data model flow (high level)

1. Collectors produce `CollectedItem` (source post + metadata)
2. Enrichment produces `EnrichedItem` (quality score + topics + research summary)
3. Candidate selection chooses which `EnrichedItem` becomes an article
4. Generation produces article body Markdown (via a selected generator)
5. Post-processing:
   - illustrations (optional)
   - markdown formatting (optional)
   - citation resolution (optional)
   - image selection/persistence (optional)
6. File write to `content/posts/*.md` (in CI), including:
   - frontmatter metadata
   - attribution block
   - references block

### Generation sequence (sync)

In `src/pipeline/orchestrator.py::generate_articles_from_enriched`:

1. Load config: `config = get_config()`
2. Create OpenAI client: `with get_openai_client(config) as client:`
3. Load generators: `generators = get_available_generators(client)`
4. Candidate selection:
   - `select_article_candidates(items)` filters enriched items
   - `select_diverse_candidates(candidates, max_articles, generators)` chooses a diverse set
5. For each selected item:
   - `generate_single_article(...)` generates content + title + slug + optional illustrations + quality score
   - `save_article_to_file(...)` writes markdown and (if enabled) performs citations + images
   - optional: `validate_article(...)` fact-check pass

### Key coupling points

- Citations are not generated by a dedicated “citation engine” at generation time; instead:
  - the *LLM is prompted* to produce inline citations in the “Author et al. (Year)” format;
  - then `save_article_to_file` tries to detect and resolve those citations post-hoc.
- Images are attached during file save (`save_article_to_file`), not during content generation.
- “Fact check” is currently a lightweight link/source reachability + structural check.

---

## 2) Enrichment engine (quality, topics, research)

Primary module: `src/enrichment/orchestrator.py`

### What enrichment does

For each `CollectedItem`:

1. Heuristic scoring (cheap, local): `src/enrichment/scorer.py::calculate_heuristic_score`
   - Uses content length, technical keywords, structure, engagement metrics fields, author patterns, etc.
   - Used for early exit below 0.15 (skip AI spend).

2. AI quality analysis (paid): `src/enrichment/ai_analyzer.py::analyze_content_quality`
   - Prompts model to emit JSON `{score, explanation}`.
   - Uses `config.enrichment_model`.
   - Retries on timeout/connection/rate limit via Tenacity.
   - On failure returns `(0.5, "Analysis failed - using default score")`.

3. Topics extraction (paid): `extract_topics_and_themes`
   - Always attempted once heuristic score ≥ 0.15.
   - On failure returns `[]`.

4. Additional research summary (paid): `research_additional_context`
   - Runs when AI score ≥ 0.3 (after topics).
   - Produces `research_summary` used by prompt templates.

### Complexity & failure modes

- Risk: “default score 0.5” on AI analysis failure
  - In `analyze_content_quality`, failures return score 0.5, which is *not* obviously “bad”.
  - That can allow low/unknown-quality items past thresholds if downstream assumes 0.5 is “okay”.
  - Suggestion: return a low score (e.g. 0.1) or attach a flag like `analysis_failed=True` so the candidate selector can penalize it.

- Threshold logic is non-intuitive
  - Heuristic is used only for early “cost gate”, but the final score is AI-based.
  - That’s fine, but it means the heuristic score’s distribution isn’t strongly tied to selection.
  - Suggestion: update docs and candidate selection to clearly treat heuristic as “budget gate”.

- Topics extraction is always attempted and can be noisy
  - Topics are used as tags and influence image queries.
  - If topic extraction returns generic or incorrect tags, it can harm SEO (irrelevant tags) and image relevance.

- Research summary is generated by the model (not retrieved)
  - This is important for accuracy: a generated “research summary” is not grounded unless it includes real sources.
  - If “research” is not backed by citations/urls, you risk plausible-sounding but wrong context.

### Recommendation: ground enrichment research

If accuracy is a priority, the primary fix is to change research enrichment from “LLM writes a summary” to:
- retrieve sources from the web (search + fetch),
- summarize those sources,
- store the URLs + quotes/snippets,
- carry them into generation so the article is anchored.

---

## 3) Article generation engine (writing, structure, quality)

### Generator selection

- Generators live under `src/generators/`.
- `select_generator(item, generators)` chooses by priority + `can_handle`.

### Prompting strategy

Key file: `src/generators/prompt_templates.py`

- Prompts instruct the model to:
  - produce ~1200–1600 words,
  - add “Key Takeaways”, define acronyms, add references,
  - include 3–7 citations in author/year format,
  - credit original source.

### Quality scoring after generation

- `src/content/quality_scorer.py` scores the generated result.
- `src/content/quality_tracker.py` records longitudinal metrics.

### Complexity & risk

- Token-cost tracking is currently hard-coded in at least one place
  - In `generate_single_article`, cost is computed using `calculate_text_cost("gpt-4o-mini", ...)`.
  - But the generator might use `config.content_model` which could differ.
  - This can skew cost accounting and any optimization decisions.

- Title and slug
  - Title generation calls `generate_article_title(...)` (uses LLM) and slug is derived.
  - There is also logic to match existing articles by slug to avoid URL churn.

- Markdown formatting
  - If `mdformat` is installed, the content is formatted; otherwise skipped.
  - This affects consistency (and can affect Google parsing of headings, lists, code blocks).

---

## 4) Citation engine (extraction, resolution, bibliography)

Primary integration point: `src/pipeline/file_io.py` (during save)

### How citations work today

1. The writer model is instructed to include inline citations like `Smith et al. (2023)`.
2. During save (`save_article_to_file`), if `config.enable_citations`:
   - `CitationExtractor.extract(article_content)` finds author/year citations via regex.
   - Each citation is resolved:
     - cache lookup (`CitationCache` in `data/citations_cache.json`)
     - else `CitationResolver.resolve(authors, year)` attempts CrossRef, then arXiv.
   - `CitationFormatter.format(...)` converts to markdown link only if resolution confidence ≥ 0.7.
   - A bibliography list is generated from resolved citations and appended under `## References`.

### Key issues / likely breaks

- The system depends on the LLM not inventing citations
  - The prompt says “don’t invent citations”, but LLMs often do.
  - When invented citations appear, they may still match regex and then:
    - resolve to the wrong paper (false positive), or
    - fail to resolve and remain as plain text.
  - Either is bad for trust and for SEO: incorrect citations can harm perceived quality.

- Resolution lookup is very weak (authors + year only)
  - CrossRef query uses `query="{authors} {year}" rows=1`.
  - With just authors + year, collision risk is high, especially for common surnames.
  - It also does not include the paper title, venue, or keywords.

- Cache is not atomic and can corrupt
  - `CitationCache._save()` writes directly; power loss/interrupt → truncated JSON.
  - Next run: cache resets silently.
  - Suggestion: use an atomic write helper (you already have `atomic_write_*` elsewhere).

- Confidence logic could drop many good citations
  - Resolver returns None if confidence < 0.7. That’s good for safety.
  - But because the query is weak, confidence may be low even for real work.

### Recommendations for accuracy

- Prefer URL-based citations (grounded)
  - Encourage the model to cite *URLs it can see* (source URLs from enrichment) rather than author/year.
  - Or switch to a “sources-first” bibliography list of URLs with titles.

- If you keep author/year citations, expand resolver inputs
  - Extract nearby context (sentence) and use it to refine CrossRef query.
  - Or generate candidate titles then validate.

- Make fact-check strict when citations are enabled
  - If `enable_citations`, require that:
    - at least N citations resolve,
    - no “citation-like” items remain unresolved above some threshold,
    - or else mark article as failed.

---

## 5) Image engine (selection, validation, attribution)

Primary components:
- `src/images/selector.py`: multi-source selection
- `src/images/library.py`: reuse strategy
- `src/images/downloader.py`: persistence of external URLs
- `src/images/cover_image.py`: pure DALL-E generation + local processing
- called from: `src/pipeline/file_io.py::save_article_to_file` when `generate_image` is enabled

### How image selection works

- `CoverImageSelector.select(title, topics, content)`
  - generates search queries (LLM)
  - tries free stock sources (Unsplash, Pexels; also claims Wikimedia in docstring)
  - validates relevance via AI (content/title vs image alt text)
  - avoids recently used images (frontmatter scan)
  - falls back to DALL-E 3 if no match found

- If a selected image is an external URL, `download_and_persist(...)` is used to store it locally.

- Frontmatter is populated with:
  - `cover.image`, `cover.alt`
  - attribution fields for photographer/source (when available)

### Key issues / complexities

- Mixed image paths and URL forms
  - `hero_path` can be a URL or a local path; code then conditionally persists.
  - This can lead to inconsistent frontmatter values across runs.

- “Wikimedia” is mentioned but verify implementation
  - The module docstring claims Wikimedia as tier 1.
  - Ensure `_search_wikimedia` exists and is actually used; if not, docs should be corrected.

- Relevance validation is model-based
  - It’s good to validate, but it introduces non-determinism and additional LLM expense.

- DALL-E fallback produces generic abstract images
  - That’s safe for branding but may reduce CTR compared to highly relevant images.

### Recommendations for SEO/CTR

- Ensure stable, descriptive `cover.alt`
  - Today it uses `article.title`. Consider adding topic words or clarifying terms.

- Prefer consistent local asset URLs
  - Always persist external images to local and store `/images/...` paths.
  - This avoids mixed-content / availability issues and speeds pages.

- Ensure attribution requirements are met (Unsplash/Pexels)
  - Many providers have attribution guidelines; store photographer + provider consistently.

---

## 6) Fact-check / validation (current state)

Module: `src/enrichment/fact_check.py` (used in generation as `validate_article`)

### What it does

- Validates source URLs are reachable (HEAD, fallback GET).
- Validates outbound markdown links are reachable.
- Checks basic structure:
  - word count bounds
  - presence of attribution block
  - presence of `## References`

### Key issues

- It does not validate factual claims
  - It cannot detect hallucinated statements, wrong numbers, wrong attributions, etc.

- Exceptions are broad
  - `validate_url_reachable` uses broad `except Exception` and does not log `exc_info=True` on warnings.

- Link checking can be slow and flaky
  - Many sites block HEAD or rate limit aggressively.
  - Retries/backoff are not currently present in this fact-check module.

### Recommendations

- Add a “claim grounding” step
  - Use citations/URLs from enrichment and require that major claims appear in the sources.
  - Or do a post-generation “verify” pass using a model that labels claims as supported/unsupported given sources.

- Treat unreachable sources as fatal
  - If the source post URL is unreachable, either skip publishing or retry later.

---

## 7) SEO + “Google safe” risk review

### Biggest risks for indexing/traffic

1. **Ungrounded content** (no real sources)
   - If the research summary is model-generated and citations are invented, Google may classify as low value.

2. **Templated / repetitive structure**
   - Prompts enforce strict structure (Key Takeaways, similar headings, etc.).
   - Too consistent can look like thin/templated AI content at scale.

3. **Weak E-E-A-T signals**
   - No author profile pages, no “about methodology”, no editorial policy.
   - Lack of primary source quotes/snippets.

4. **Broken links / low-quality references**
   - Broken references are a quality signal problem.

5. **Duplicate/near-duplicate content**
   - Dedup exists, but semantic drift can still produce “same story different week”.

### Practical improvements (high leverage)

- Make enrichment fetch real sources (URLs) and keep them in the `EnrichedItem`.
- Require citations to be URLs from those sources or from an explicit retrieval step.
- Add a post-generation “verification” prompt that checks:
  - each paragraph in the article is supported by at least one source URL.
- Add per-article uniqueness checks (semantic dedup vs recent articles) before saving.
- Add structured data and canonical URLs in Hugo templates (outside Python scope).

---

## 8) Recommended next fixes (prioritized)

1. **Grounding-first citations**
   - Stop relying on LLM author/year citations; instead build a bibliography from fetched URLs.

2. **Fix cost model mismatch**
   - Ensure cost estimation uses `config.content_model` (and title model) rather than hard-coded.

3. **Make citation cache atomic**
   - Use atomic JSON write to prevent corruption.

4. **Harden fact-check**
   - Add retry/backoff; log stack traces; optionally fail generation when validation fails.

5. **Stabilize image pipeline**
   - Always persist selected image locally; make frontmatter consistent.

---

## Appendix: quick pointers

- Generation orchestration: `src/pipeline/orchestrator.py`
- File save + citations + images: `src/pipeline/file_io.py`
- Enrichment: `src/enrichment/orchestrator.py`, `src/enrichment/ai_analyzer.py`, `src/enrichment/scorer.py`
- Citations: `src/citations/extractor.py`, `src/citations/resolver.py`, `src/citations/cache.py`
- Images: `src/images/selector.py`, `src/images/library.py`, `src/images/downloader.py`, `src/images/cover_image.py`
