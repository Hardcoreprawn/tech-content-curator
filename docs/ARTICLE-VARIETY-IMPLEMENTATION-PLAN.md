# Article Variety Implementation Plan
**Status:** Planning Phase  
**Goal:** Add voice variety, length flexibility, and structural diversity while maintaining quality  
**Approach:** Iterative, testable phases with OpenAI API only  
**Date:** November 4, 2025

---

## Overview

Transform article generation from monotonous single-voice output to a diverse content ecosystem with:
- **7 distinct writing voices** with personality and style
- **Dynamic length selection** (600-4000+ words based on content depth)
- **Varied structures and hooks** to prevent reader fatigue
- **Automated quality feedback loop** for continuous improvement
- **Smart voice rotation** with guardrails to prevent repetition

### Core Principles
âœ… **Iterative:** Each phase is independently testable and deployable  
âœ… **Automated:** Minimal manual intervention, AI-driven decisions  
âœ… **Quality-first:** Variety should enhance, not degrade, article quality  
âœ… **Cost-conscious:** Use GPT-4o-mini for most decisions, reserve GPT-4o/turbo for deep-dives  
âœ… **Measurable:** Track metrics at each phase to validate improvements  

---

## Phase 1: Voice Profile System (Week 1-2)
**Goal:** Implement 7 distinct writing voices with automatic selection  
**Complexity:** Medium  
**Cost Impact:** Minimal (voice selection uses GPT-4o-mini)

### 1.1: Define Voice Profiles (Day 1-2)

**Files to Create:**
```
src/generators/voices/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ profiles.py           # Voice profile definitions
â”œâ”€â”€ selector.py           # Voice selection logic
â””â”€â”€ rotation_tracker.py   # Prevent voice repetition
```

**Voice Profiles to Implement:**

| Voice ID | Name | Style | Temperature | Best For | Avoid |
|----------|------|-------|-------------|----------|-------|
| `technical_explainer` | The Technical Explainer | Formal, educational, balanced | 0.5 | Tutorials, technical deep-dives | News, opinion pieces |
| `storyteller` | The Storyteller | Narrative, conversational, anecdotal | 0.7 | Historical pieces, case studies | Pure technical how-tos |
| `analyst` | The Analyst | Sharp, critical, questions assumptions | 0.6 | Comparisons, reviews, trade-offs | Beginner tutorials |
| `enthusiast` | The Enthusiast | Passionate, energetic, optimistic | 0.7 | New releases, innovations | Serious security topics |
| `pragmatist` | The Pragmatist | Code-first, action-oriented, practical | 0.5 | Implementation guides, DevOps | Abstract theory |
| `researcher` | The Researcher | Dense, citation-heavy, methodical | 0.4 | Scientific articles, academic | Quick news updates |
| `journalist` | The Journalist | Inverted pyramid, punchy, fact-dense | 0.6 | Breaking news, announcements | Long-form analysis |

**Voice Profile Data Structure:**
```python
@dataclass
class VoiceProfile:
    voice_id: str
    name: str
    temperature: float
    system_message: str           # Sets overall tone
    style_guidance: str           # Specific writing instructions
    opening_hook_styles: List[str]  # ["question", "stat", "story", "bold_statement"]
    preferred_structures: List[str]  # Structure template IDs
    min_citations: int            # Minimum citations for this voice
    max_word_count: int           # Natural ceiling for this voice
    paragraph_style: str          # "short_punchy", "flowing", "mixed"
    banned_phrases: List[str]     # Overused words/phrases to avoid
```

**Voice Selection Algorithm:**
```python
def select_voice(item: EnrichedItem, recent_voices: List[str]) -> VoiceProfile:
    """
    Intelligent voice selection with variety guardrails.
    
    Logic:
    1. Determine content type (tutorial, news, analysis, research, general)
    2. Get compatible voices for that content type (2-3 voices typically)
    3. Apply recency filter: exclude last 2 voices used
    4. Score remaining voices based on:
       - Topic fit (0-1.0)
       - Complexity match (0-1.0)
       - Variety bonus (0.2 if voice hasn't been used in 5+ articles)
    5. Add controlled randomness (Â±10% score variation)
    6. Select highest scoring voice
    
    Uses: gpt-4o-mini for selection (cheap, fast)
    """
```

**Testing Strategy:**
- Generate 20 articles, track voice distribution
- Ensure no voice appears 2x consecutively
- Verify voice-content type compatibility
- Manual review: Do voices feel distinct?

**Success Metrics:**
- Voice distribution: No voice >30% in 20-article batch
- Zero consecutive duplicates
- Subjective quality: 3+ reviewers confirm voices feel different

---

### 1.2: Integrate Voice Selection into Pipeline (Day 3-4)

**Files to Modify:**
```
src/pipeline/article_generation.py  # Add voice selection step
src/generators/general.py           # Use selected voice profile
src/models.py                       # Add voice metadata to GeneratedArticle
```

**Changes:**
```python
# In GeneratedArticle model
class GeneratedArticle(BaseModel):
    # ... existing fields ...
    voice_profile: str = Field(default="technical_explainer")  # NEW
    voice_metadata: dict = Field(default_factory=dict)         # NEW
```

**Pipeline Integration:**
```python
def generate_single_article(item: EnrichedItem, client: OpenAI) -> GeneratedArticle:
    # NEW: Select voice before generation
    recent_voices = get_recent_voice_history(limit=3)
    voice_profile = select_voice(item, recent_voices)
    
    # Pass voice to generator
    generator = select_generator(item)
    content, tokens_in, tokens_out = generator.generate_content(
        item, 
        voice_profile=voice_profile  # NEW parameter
    )
    
    # Track voice in article metadata
    article.voice_profile = voice_profile.voice_id
    article.voice_metadata = {
        "voice_name": voice_profile.name,
        "temperature": voice_profile.temperature,
        "selection_reason": voice_profile.selection_reason
    }
```

**Voice Rotation Tracking:**
```python
# Store in data/voice_history.json
{
    "history": [
        {"article_slug": "...", "voice_id": "storyteller", "date": "2025-11-04"},
        {"article_slug": "...", "voice_id": "analyst", "date": "2025-11-04"},
        # ... last 20 articles
    ]
}
```

**Testing:**
- Run full pipeline with voice selection enabled
- Generate 10 articles, verify voice metadata is saved
- Check `voice_history.json` is updated correctly
- Verify voices rotate as expected

---

### 1.3: Voice-Specific Prompt Templates (Day 5-7)

**Files to Modify:**
```
src/generators/prompt_templates.py  # Add voice-specific prompts
src/generators/general.py           # Use voice prompts
```

**Implementation:**
Each voice gets custom prompt sections:

```python
VOICE_PROMPTS = {
    "storyteller": """
STORYTELLING VOICE INSTRUCTIONS:
- Open with a vivid anecdote or concrete scenario that readers can picture
- Use narrative arc: setup â†’ complication â†’ resolution
- Include dialogue or quotes when relevant (e.g., "As the Gmail team put it...")
- Vary sentence length: Mix short punchy sentences with longer flowing ones
- Use transitional phrases: "But here's where it gets interesting...", "Fast forward to..."
- Make abstract concepts concrete with real-world examples
- End sections with forward momentum, not just summaries

OPENING HOOK STYLES (rotate these):
1. Scene-setting: "Picture a developer in 2004, frustrated with 2MB of Hotmail storage..."
2. Personal story: "When I first encountered this problem..."
3. Historical moment: "On April 1, 2004, something changed in the email world..."

BANNED PHRASES: "Let's explore", "It's important to understand", "In conclusion"
""",
    
    "analyst": """
ANALYTICAL VOICE INSTRUCTIONS:
- Lead with your thesis: State your conclusion upfront, then prove it
- Question conventional wisdom: "Most developers think X, but data shows Y"
- Use comparison frameworks: "When evaluating X vs Y, three factors matter..."
- Integrate data naturally: Don't just list stats, explain their implications
- Present counterarguments, then refute or acknowledge them
- Use "However...", "The problem with this approach...", "What's often missed..."
- Be opinionated but fair: Strong views, backed by evidence

STRUCTURE:
1. Bold opening claim
2. Why conventional wisdom is incomplete/wrong
3. Data-backed analysis with 3-5 key points
4. Trade-offs and edge cases
5. Actionable recommendations

BANNED PHRASES: "Simply put", "Let's look at", "In summary", "As we've seen"
""",
    
    "pragmatist": """
PRAGMATIST VOICE INSTRUCTIONS:
- Get to the point immediately: No long introductions
- Code and examples first, theory second
- Use imperatives: "Clone the repo", "Run these commands", "Set this flag"
- Short paragraphs (2-4 sentences max)
- Anticipate common errors: "If you see X, it means Y"
- Include "gotchas" and "pro tips" sections
- Show, don't tell: Prefer code blocks over explanations

STRUCTURE:
1. One-sentence problem statement
2. Prerequisites (bullet list)
3. Step-by-step implementation with code
4. Common issues & fixes
5. Next steps (1-2 sentences)

BANNED PHRASES: "One must consider", "It's worth noting", "Interestingly", "Furthermore"
""",
    
    # ... (similar detailed prompts for other voices)
}
```

**Testing:**
- Generate 1 article per voice (7 total)
- Compare side-by-side: Do they feel genuinely different?
- Check for banned phrase usage
- Verify structural differences

**Success Criteria:**
- Blind test: 3 reviewers can identify voice from style alone (>70% accuracy)
- Zero banned phrases in generated content
- Each voice follows its structural template

---

## Phase 2: Dynamic Length System (Week 3-4)
**Goal:** Break the 1000-1600 word monotony with intelligent length selection  
**Complexity:** Medium  
**Cost Impact:** Low (selection is cheap; longer articles cost more tokens)

### 2.1: Length Profile System (Day 1-2)

**Files to Create:**
```
src/generators/length/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ profiles.py       # Length definitions
â””â”€â”€ selector.py       # Length selection logic
```

**Length Profiles:**

```python
class ArticleLength(Enum):
    QUICK_HIT = "quick_hit"         # 600-900 words
    STANDARD = "standard"           # 1200-1600 words (current default)
    EXTENDED = "extended"           # 1800-2500 words
    DEEP_DIVE = "deep_dive"         # 2500-4000 words
    EPIC = "epic"                   # 4000-6000+ words

@dataclass
class LengthProfile:
    length_id: ArticleLength
    word_range: tuple[int, int]
    max_tokens: int                 # For OpenAI API
    model_preference: str           # "gpt-4o-mini", "gpt-4o", "gpt-4-turbo"
    min_citations: int
    recommended_sections: int
    diagram_encouragement: str      # Prompt addition for visual elements
```

**Length Profiles Definition:**
```python
LENGTH_PROFILES = {
    ArticleLength.QUICK_HIT: LengthProfile(
        length_id=ArticleLength.QUICK_HIT,
        word_range=(600, 900),
        max_tokens=1200,
        model_preference="gpt-4o-mini",
        min_citations=2,
        recommended_sections=3,
        diagram_encouragement="Keep explanations concise; save space for key points."
    ),
    
    ArticleLength.DEEP_DIVE: LengthProfile(
        length_id=ArticleLength.DEEP_DIVE,
        word_range=(2500, 4000),
        max_tokens=5000,
        model_preference="gpt-4o",  # Better model for complex content
        min_citations=8,
        recommended_sections=7,
        diagram_encouragement="""
        Include 2-3 detailed diagrams or data visualizations:
        - ASCII diagrams for architecture
        - Mermaid charts for flows/sequences
        - Data tables for comparisons
        Explain complex concepts with step-by-step breakdowns.
        """
    ),
    
    ArticleLength.EPIC: LengthProfile(
        length_id=ArticleLength.EPIC,
        word_range=(4000, 6000),
        max_tokens=7500,
        model_preference="gpt-4-turbo",  # Reserve for truly special content
        min_citations=12,
        recommended_sections=10,
        diagram_encouragement="""
        This is a comprehensive guide. Include:
        - 3-5 diagrams (Mermaid, ASCII art, or description for manual creation)
        - Multiple code examples with explanations
        - Data tables and benchmark comparisons
        - Case studies or real-world examples
        - "Further Reading" section with 5-10 resources
        """
    ),
}
```

### 2.2: Complexity Scoring for Length Selection (Day 3-5)

**Add to enrichment stage:**

```python
# In src/enrichment/analyzer.py

def calculate_content_complexity(item: EnrichedItem) -> float:
    """
    Score content complexity (0.0 - 1.0) to guide length selection.
    
    Factors:
    - Research summary length and depth (0.3 weight)
    - Number of topics covered (0.15 weight)
    - Presence of technical terms/jargon (0.15 weight)
    - Source credibility (academic papers score higher) (0.2 weight)
    - Content type (research/analysis score higher) (0.2 weight)
    
    Returns:
        Complexity score where:
        - 0.0-0.3: Simple, suitable for quick hits
        - 0.3-0.6: Standard complexity
        - 0.6-0.8: Deserves extended treatment
        - 0.8-1.0: Deep-dive or epic worthy
    """
    score = 0.0
    
    # Research summary depth
    if len(item.research_summary) > 1000:
        score += 0.3
    elif len(item.research_summary) > 500:
        score += 0.15
    
    # Topic breadth
    topic_score = min(len(item.topics) / 8, 1.0) * 0.15
    score += topic_score
    
    # Technical density (heuristic: check for complex terms)
    technical_terms = count_technical_terms(item.research_summary)
    tech_score = min(technical_terms / 20, 1.0) * 0.15
    score += tech_score
    
    # Source type
    if "arxiv" in str(item.original.url) or "doi.org" in str(item.original.url):
        score += 0.2
    elif item.quality_score > 0.7:
        score += 0.1
    
    # Content type complexity
    content_type_scores = {
        "research": 0.2,
        "analysis": 0.15,
        "tutorial": 0.1,
        "news": 0.05,
        "general": 0.1
    }
    content_type = detect_content_type(item)
    score += content_type_scores.get(content_type, 0.1)
    
    return min(score, 1.0)
```

**Length Selection Algorithm:**

```python
def select_article_length(
    item: EnrichedItem, 
    complexity_score: float,
    voice_profile: VoiceProfile,
    budget_remaining: float
) -> LengthProfile:
    """
    Intelligent length selection with cost awareness.
    
    Decision tree:
    1. Check complexity score
    2. Check voice preferences (some voices prefer shorter)
    3. Check budget (reserve expensive GPT-4 for worthy content)
    4. Apply variety bonus (encourage occasional deep-dives)
    5. Add controlled randomness (Â±1 length tier 20% of time)
    
    Distribution targets (over 100 articles):
    - QUICK_HIT: 15%
    - STANDARD: 50%
    - EXTENDED: 25%
    - DEEP_DIVE: 8%
    - EPIC: 2%
    """
    
    # Base selection from complexity
    if complexity_score < 0.3:
        base_length = ArticleLength.QUICK_HIT
    elif complexity_score < 0.6:
        base_length = ArticleLength.STANDARD
    elif complexity_score < 0.8:
        base_length = ArticleLength.EXTENDED
    elif complexity_score < 0.95:
        base_length = ArticleLength.DEEP_DIVE
    else:
        base_length = ArticleLength.EPIC
    
    # Voice constraints (some voices don't suit all lengths)
    if voice_profile.voice_id == "journalist" and base_length > ArticleLength.STANDARD:
        base_length = ArticleLength.STANDARD  # Journalists write concisely
    
    if voice_profile.voice_id == "researcher" and base_length == ArticleLength.QUICK_HIT:
        base_length = ArticleLength.STANDARD  # Researchers need space
    
    # Budget gating for expensive models
    length_profile = LENGTH_PROFILES[base_length]
    if length_profile.model_preference in ["gpt-4o", "gpt-4-turbo"]:
        estimated_cost = estimate_generation_cost(length_profile)
        if estimated_cost > budget_remaining:
            # Downgrade to cheaper alternative
            base_length = ArticleLength.STANDARD
            console.print("[yellow]âš [/yellow] Budget constraint: downgrading to standard length")
    
    # Variety injection: 20% chance to shift Â±1 tier (but not below/above limits)
    if random.random() < 0.2:
        tier_shift = random.choice([-1, 1])
        # ... (implementation to shift within enum bounds)
    
    return LENGTH_PROFILES[base_length]
```

**Testing:**
- Score complexity for existing articles (manual validation)
- Generate articles at each length tier
- Verify length targets are hit (Â±10%)
- Check that deep-dives actually deserve the length

**Success Metrics:**
- 90% of articles hit their target word range (Â±15%)
- Deep-dive articles score >0.8 complexity upon review
- Cost per article increases <30% (due to occasional GPT-4 usage)

---

### 2.3: Model Routing (Day 6-7)

**Implement smart model selection:**

```python
# In src/generators/base.py

def select_model_for_article(
    length_profile: LengthProfile,
    voice_profile: VoiceProfile,
    config: Config
) -> str:
    """
    Route to appropriate model based on length and complexity.
    
    Default routing:
    - QUICK_HIT, STANDARD: gpt-4o-mini (cheap, fast)
    - EXTENDED: gpt-4o-mini (still handles well)
    - DEEP_DIVE: gpt-4o (better reasoning for complex content)
    - EPIC: gpt-4-turbo or gpt-4o (best quality for flagship content)
    
    Cost-conscious override: Use env var MAX_MODEL_TIER to cap spending
    """
    preferred = length_profile.model_preference
    
    # Check config limits
    max_tier = config.max_model_tier  # "mini", "standard", "turbo"
    
    if max_tier == "mini":
        return "gpt-4o-mini"
    elif max_tier == "standard" and "turbo" in preferred:
        return "gpt-4o"
    
    return preferred
```

**Add to config:**
```python
# In src/config.py
MAX_MODEL_TIER: str = os.getenv("MAX_MODEL_TIER", "standard")  # "mini" | "standard" | "turbo"
DEEP_DIVE_BUDGET_PER_RUN: float = float(os.getenv("DEEP_DIVE_BUDGET", "0.50"))  # $0.50 per pipeline run
```

---

## Phase 3: A/B Testing & Quality Feedback Loop (Week 5-6)
**Goal:** Automated quality assessment and continuous improvement  
**Complexity:** High  
**Cost Impact:** Low (uses GPT-4o-mini for reviews)

### 3.1: Reviewer AI System (Day 1-3)

**Files to Create:**
```
src/quality/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ reviewer.py           # AI-powered article review
â”œâ”€â”€ comparator.py         # A/B comparison logic
â””â”€â”€ feedback_tracker.py   # Store and learn from reviews
```

**Reviewer Implementation:**

```python
class ArticleReviewer:
    """
    AI-powered article quality and variety assessment.
    Uses GPT-4o-mini for cost-effective reviews.
    """
    
    def review_article(
        self, 
        article: GeneratedArticle,
        content: str,
        voice_profile: VoiceProfile
    ) -> ReviewResult:
        """
        Comprehensive article review across multiple dimensions.
        
        Review criteria:
        1. Voice consistency (does it match the intended voice?)
        2. Structural variety (is it formulaic or interesting?)
        3. Engagement (compelling hooks, good pacing?)
        4. Technical accuracy (are citations appropriate?)
        5. Readability (matches target audience?)
        6. Originality (fresh perspective or generic?)
        
        Returns scores (0-10) for each dimension + overall assessment
        """
        
        prompt = f"""
You are an expert content reviewer evaluating a tech blog article.

ARTICLE METADATA:
- Voice: {voice_profile.name}
- Length: {article.word_count} words
- Content Type: {article.content_type}
- Topics: {', '.join(article.tags[:5])}

VOICE EXPECTATIONS:
{voice_profile.style_guidance}

ARTICLE CONTENT:
{content[:2000]}  # First 2000 chars for review

EVALUATION CRITERIA (score each 0-10):

1. **Voice Consistency**: Does the writing match the expected voice profile?
   - Look for: tone, sentence structure, vocabulary, perspective
   - Red flags: generic language, banned phrases, inconsistent style

2. **Structural Variety**: Is the structure interesting or formulaic?
   - Good: unexpected sections, varied heading styles, creative organization
   - Bad: "Introduction â†’ Body â†’ Conclusion" template, predictable headers

3. **Opening Hook**: Does it grab attention in the first 2-3 sentences?
   - Rate: weak generic opening (0-3), decent (4-6), compelling (7-10)

4. **Pacing & Engagement**: Does the article maintain interest?
   - Check: paragraph variation, transitions, rhythm, content density

5. **Technical Depth**: Appropriate depth for the topic and audience?
   - Too shallow, just right, or unnecessarily complex?

6. **Citation Quality**: Are citations natural and credible?
   - Look for: forced citations, made-up references, good integration

7. **Originality**: Fresh perspective or generic rehash?
   - Does it add value beyond summarizing the source?

8. **Readability**: Clear and accessible writing?
   - Check: jargon explanation, logical flow, sentence clarity

Return JSON:
{
  "scores": {
    "voice_consistency": 0-10,
    "structural_variety": 0-10,
    "opening_hook": 0-10,
    "pacing": 0-10,
    "technical_depth": 0-10,
    "citation_quality": 0-10,
    "originality": 0-10,
    "readability": 0-10
  },
  "overall_score": 0-10,
  "strengths": ["strength 1", "strength 2", ...],
  "weaknesses": ["weakness 1", "weakness 2", ...],
  "recommended_improvements": ["improvement 1", ...],
  "should_regenerate": true/false,
  "confidence": "high" | "medium" | "low"
}
"""
        
        # Use GPT-4o-mini for cost-effective reviews
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            max_tokens=1000,
            temperature=0.3,
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}  # Ensure JSON response
        )
        
        return ReviewResult.from_json(response.choices[0].message.content)
```

**Integration into Pipeline:**

```python
def generate_single_article_with_review(
    item: EnrichedItem,
    client: OpenAI,
    max_retries: int = 1
) -> tuple[GeneratedArticle, ReviewResult]:
    """
    Generate article with optional regeneration based on AI review.
    
    Flow:
    1. Generate article (normal process)
    2. Run AI review
    3. If overall_score < 6.0 AND should_regenerate=True:
       - Log failure reasons
       - Attempt regeneration with feedback (up to max_retries)
    4. Return best version + review
    """
    
    for attempt in range(max_retries + 1):
        article = generate_single_article(item, client)
        review = reviewer.review_article(article, article.content, article.voice_profile)
        
        if review.overall_score >= 6.0 or attempt == max_retries:
            # Acceptable quality or out of retries
            break
        
        console.print(f"[yellow]âš [/yellow] Article scored {review.overall_score}/10, regenerating...")
        # Add feedback to prompt for next iteration
        feedback_prompt = build_improvement_prompt(review)
        # ... regenerate with feedback ...
    
    return article, review
```

**Testing:**
- Review 20 existing articles to calibrate reviewer
- Compare AI reviews with human assessments (correlation check)
- Test regeneration logic with intentionally poor articles

---

### 3.2: A/B Comparison System (Day 4-5)

**For major changes, compare old vs new approaches:**

```python
class ABComparator:
    """
    Compare two article generation approaches.
    Useful for validating new voices, structures, or prompts.
    """
    
    def compare_approaches(
        self,
        item: EnrichedItem,
        approach_a: GenerationConfig,  # e.g., old voice system
        approach_b: GenerationConfig,  # e.g., new voice system
        num_samples: int = 5
    ) -> ComparisonReport:
        """
        Generate articles with both approaches and compare.
        
        Uses blind review: AI reviewer doesn't know which is A or B.
        """
        
        results_a = [generate_with_config(item, approach_a) for _ in range(num_samples)]
        results_b = [generate_with_config(item, approach_b) for _ in range(num_samples)]
        
        # Blind review
        all_articles = results_a + results_b
        random.shuffle(all_articles)
        reviews = [reviewer.review_article(art) for art in all_articles]
        
        # Aggregate scores
        avg_score_a = mean([r.overall_score for r in reviews if r.article_id in results_a])
        avg_score_b = mean([r.overall_score for r in reviews if r.article_id in results_b])
        
        # Statistical significance test
        p_value = stats.ttest_ind(scores_a, scores_b).pvalue
        
        return ComparisonReport(
            approach_a_score=avg_score_a,
            approach_b_score=avg_score_b,
            winner="B" if avg_score_b > avg_score_a else "A",
            significance=p_value,
            recommendation="Deploy B" if avg_score_b > avg_score_a and p_value < 0.05 else "Keep A"
        )
```

**Use cases:**
- Validate new voice profiles before deployment
- Compare GPT-4o-mini vs GPT-4o for quality vs cost
- Test new structural templates

---

### 3.3: Feedback Loop & Continuous Learning (Day 6-7)

**Store review data for learning:**

```python
# data/quality_feedback.json (already exists, extend it)
{
    "reviews": [
        {
            "article_slug": "...",
            "voice_id": "storyteller",
            "length": "deep_dive",
            "review": { /* ReviewResult */ },
            "generated_at": "2025-11-04T...",
            "model_used": "gpt-4o"
        }
    ],
    "patterns": {
        "voice_performance": {
            "storyteller": {"avg_score": 8.2, "count": 15},
            "analyst": {"avg_score": 7.8, "count": 12},
            // ...
        },
        "length_performance": {
            "deep_dive": {"avg_score": 8.5, "count": 8},
            // ...
        },
        "common_weaknesses": [
            {"issue": "generic_opening", "frequency": 0.3},
            {"issue": "formulaic_structure", "frequency": 0.25}
        ]
    }
}
```

**Use feedback to improve:**

```python
def get_adaptive_voice_guidance(voice_profile: VoiceProfile) -> str:
    """
    Enhance voice prompts based on past review feedback.
    
    Example: If "storyteller" voice consistently gets dinged for
    "weak opening hooks", add extra emphasis in prompt.
    """
    
    feedback = load_voice_feedback(voice_profile.voice_id)
    
    if feedback.common_weaknesses.contains("weak_opening"):
        extra_guidance = """
        CRITICAL: Opening hook must be compelling. Avoid:
        - Generic questions like "What is X?"
        - Bland statements like "In today's tech world..."
        
        Instead, use:
        - Vivid scenes: "When Sarah deployed her first Kubernetes cluster, everything broke."
        - Surprising stats: "73% of developers believe CPU cache myths..."
        - Bold claims: "The way you're using git is probably wrong."
        """
        return extra_guidance
    
    return ""
```

**Automated improvement cycle:**
1. Generate articles â†’ 2. Review â†’ 3. Store feedback â†’ 4. Analyze patterns â†’ 5. Update prompts â†’ (repeat)

---

## Phase 4: Structural & Formatting Variety (Week 7-8)
**Goal:** Break structural monotony with diverse templates and formatting  
**Complexity:** Medium  
**Cost Impact:** Minimal (variety comes from prompts, not model usage)

### 4.1: Hook Variety System (Day 1-2)

**Implement varied opening hooks:**

```python
OPENING_HOOKS = {
    "question": """Start with a thought-provoking question that challenges assumptions.
    Example: "What if everything you know about CPU caches is wrong?"
    """,
    
    "stat": """Open with a surprising statistic or data point.
    Example: "In 2004, Gmail launched with 1GB of storageâ€”500x more than Hotmail's 2MB."
    """,
    
    "story": """Begin with a narrative scene or anecdote.
    Example: "Picture a developer at 3am, staring at a deployment that just failed for the tenth time."
    """,
    
    "bold_statement": """Lead with a strong, opinionated claim.
    Example: "Docker changed how we deploy software, but it also made our systems more fragile."
    """,
    
    "contrast": """Set up a before/after or expectation vs reality contrast.
    Example: "Most developers think X, but the data tells a different story."
    """,
    
    "scenario": """Paint a concrete scenario readers can relate to.
    Example: "You've just inherited a codebase with 47 environment variables and zero documentation."
    """
}
```

**Hook Selection:**
- Rotate through hooks within each voice
- Track last 5 hooks used per voice
- Select least-recently-used hook compatible with content type

---

### 4.2: Structure Template Library (Day 3-5)

**Create 8-10 distinct structural templates:**

```python
STRUCTURE_TEMPLATES = {
    "classic": {
        "name": "Classic Essay",
        "sections": [
            "Introduction with hook",
            "Key Takeaways (bullets)",
            "Background/Context",
            "Main Content (3-5 sections)",
            "Practical Implications",
            "Conclusion"
        ],
        "suitable_for": ["general", "tutorial", "analysis"]
    },
    
    "inverted_pyramid": {
        "name": "News/Journalist Style",
        "sections": [
            "Lede (most important info first)",
            "Key Facts (bullet list)",
            "Detailed Explanation",
            "Background Context",
            "Expert Quotes/Reactions",
            "What's Next"
        ],
        "suitable_for": ["news", "announcements"]
    },
    
    "problem_solution": {
        "name": "Problem-Solution Framework",
        "sections": [
            "The Problem (with examples)",
            "Why It Matters",
            "Solution Overview",
            "Implementation Details",
            "Pitfalls to Avoid",
            "Real-World Results"
        ],
        "suitable_for": ["tutorial", "technical", "pragmatist voice"]
    },
    
    "comparison_matrix": {
        "name": "Comparison/Analysis Format",
        "sections": [
            "Introduction to Problem Space",
            "Evaluation Criteria",
            "Option A: Deep Dive",
            "Option B: Deep Dive",
            "Option C: Deep Dive (if applicable)",
            "Side-by-Side Comparison Table",
            "Decision Matrix: When to Use Each",
            "Recommendations"
        ],
        "suitable_for": ["analysis", "reviews", "analyst voice"]
    },
    
    "narrative_arc": {
        "name": "Story-Driven Article",
        "sections": [
            "Opening Scene",
            "Setup: Why This Matters",
            "Rising Action: The Challenge",
            "Climax: The Breakthrough/Revelation",
            "Resolution: How It Works",
            "Epilogue: Broader Implications"
        ],
        "suitable_for": ["historical", "case studies", "storyteller voice"]
    },
    
    "deep_research": {
        "name": "Academic/Research Format",
        "sections": [
            "Abstract/Summary",
            "Background & Context",
            "Methodology/Approach",
            "Findings (with subsections)",
            "Analysis & Discussion",
            "Limitations",
            "Future Directions",
            "Conclusion",
            "References"
        ],
        "suitable_for": ["research", "scientific", "deep-dive length"]
    },
    
    "qa_format": {
        "name": "Q&A Explainer",
        "sections": [
            "Introduction: Why You Should Care",
            "What is X?",
            "Why Does X Matter?",
            "How Does X Work?",
            "When Should You Use X?",
            "What Are the Trade-offs?",
            "What's Next?"
        ],
        "suitable_for": ["explainers", "introductory content"]
    },
    
    "builders_guide": {
        "name": "Implementation Guide",
        "sections": [
            "What We're Building",
            "Prerequisites",
            "Architecture Overview",
            "Step 1: Setup",
            "Step 2-N: Implementation",
            "Testing & Validation",
            "Gotchas & Troubleshooting",
            "Deployment Checklist",
            "Next Steps"
        ],
        "suitable_for": ["tutorials", "pragmatist voice", "technical"]
    }
}
```

**Structure Selection Logic:**
```python
def select_structure(
    voice_profile: VoiceProfile,
    content_type: str,
    length_profile: LengthProfile,
    recent_structures: List[str]
) -> dict:
    """
    Choose structure template with variety guardrails.
    
    Priority:
    1. Must be suitable for content type
    2. Must be compatible with voice profile
    3. Exclude last 3 structures used
    4. Prefer structures that fit length (deep-dive â†’ research format)
    5. Add variety bonus for rarely-used structures
    """
```

---

### 4.3: Visual Element Encouragement (Day 6-8)

**Prompt additions for richer formatting:**

```python
VISUAL_PROMPTS = {
    "code_heavy": """
    Include 4-6 code examples throughout:
    - Use proper syntax highlighting markers (```python, ```javascript, etc.)
    - Add inline comments explaining key lines
    - Show before/after comparisons when relevant
    - Include command-line examples with expected output
    """,
    
    "diagram_rich": """
    Include 2-3 visual representations:
    
    1. ASCII diagrams for architecture:
    ```
    [Browser] --> [CDN] --> [Load Balancer]
                               |
                         +-----+-----+
                         |           |
                    [Server 1]  [Server 2]
                         |           |
                      [Database Cluster]
    ```
    
    2. Mermaid charts for flows:
    ```mermaid
    graph TD
        A[User Request] --> B{Cache Hit?}
        B -->|Yes| C[Return Cached]
        B -->|No| D[Query Database]
    ```
    
    3. Tables for comparisons (markdown tables)
    """,
    
    "data_driven": """
    Present data visually:
    - Use markdown tables for structured comparisons
    - Include benchmark results with context
    - Show trends with ASCII charts if appropriate
    - Provide data sources for all statistics
    """
}
```

**Implementation:**
Add visual prompts to long-form articles (extended+) and specific voices (pragmatist, analyst, researcher)

---

## Phase 5: Pacing & Perspective Experiments (Week 9-10)
**Goal:** Fine-tune article rhythm and experiment with perspective  
**Complexity:** Low-Medium  
**Cost Impact:** Minimal

### 5.1: Pacing Control (Day 1-3)

**Add pacing instructions to voice profiles:**

```python
PACING_STYLES = {
    "fast_punchy": """
    Pacing: FAST
    - Short paragraphs (2-4 sentences max)
    - Vary sentence length: mix 5-word and 20-word sentences
    - Use short transitional phrases
    - No long-winded explanations
    - Get to the point quickly
    """,
    
    "flowing_contemplative": """
    Pacing: MEASURED
    - Longer paragraphs (5-7 sentences) for deep exploration
    - Build ideas gradually with supporting details
    - Use longer transitional sentences
    - Allow space for nuance and complexity
    """,
    
    "mixed_dynamic": """
    Pacing: VARIED
    - Mix paragraph lengths (2-7 sentences)
    - Alternate between quick points and deeper exploration
    - Use pacing shifts to maintain engagement:
      * Short burst for emphasis
      * Longer paragraph for complex explanation
      * Back to short for conclusion
    - Think of it as musical rhythm: fast â†’ slow â†’ fast
    """
}
```

**Testing:**
- Generate articles with each pacing style
- Measure average paragraph length, sentence length
- Subjective review: Does pacing feel appropriate?

---

### 5.2: Perspective Experiments (Day 4-5)

**Test different narrative perspectives carefully:**

```python
PERSPECTIVE_OPTIONS = {
    "second_person": {
        "style": "You/Your",
        "example": "When you deploy to production, you need to consider...",
        "suitable_for": ["tutorials", "how-to", "pragmatist voice"],
        "confidence": "high"  # This works well, current default
    },
    
    "first_person_occasional": {
        "style": "We/Our (inclusive)",
        "example": "Let's explore how we can optimize...",
        "suitable_for": ["collaborative tone", "explorations"],
        "confidence": "medium"
    },
    
    "third_person_objective": {
        "style": "The developer/Engineers/Researchers",
        "example": "Developers face challenges when optimizing cache performance...",
        "suitable_for": ["news", "objective analysis", "research summaries"],
        "confidence": "medium",  # Needs testing
        "caution": "Can feel distant; use sparingly"
    }
}
```

**Recommendation:**
- Keep 2nd person as primary (80% of articles)
- Use 1st person inclusive for ~15% (storyteller, enthusiast voices)
- Experiment with 3rd person objective only for news/research (5%)
- Track reader engagement if possible (time on page, bounce rate)

---

## Phase 6: Banned Phrase Filter & Title Variety (Week 11)
**Goal:** Eliminate overused phrases and create diverse titles  
**Complexity:** Low  
**Cost Impact:** Minimal (just prompt engineering)

### 6.1: Overused Verb Elimination (Day 1-2)

**Create banned phrase system:**

```python
BANNED_TITLE_PATTERNS = {
    "overused_verbs": [
        "Unlocking",
        "Mastering",
        "Revolutionizing",
        "Transforming",
        "Unveiling",
        "Unpacking",
        "Harnessing",
        "Navigating",
        "Elevating",
        "Empowering"
    ],
    
    "generic_structures": [
        "X: A Guide",
        "X: What You Need to Know",
        "The Ultimate Guide to X",
        "X Made Easy",
        "X for Beginners"
    ],
    
    "clickbait": [
        "You Won't Believe",
        "This One Trick",
        "X Will Shock You",
        "The Secret to"
    ]
}

TITLE_VARIETY_GUIDANCE = """
TITLE REQUIREMENTS:
- Under 60 characters
- NO banned verbs: {banned_verbs}
- NO generic structures: {banned_structures}
- BE specific and concrete
- USE active, descriptive language

GOOD TITLE PATTERNS:
- Direct benefit: "How Gmail Went from 2MB to 15GB"
- Surprising fact: "CPU Caches Don't Work the Way You Think"
- Controversy/Question: "Is Docker Making Your System Worse?"
- Technical specificity: "Implementing Zero-Downtime Postgres Migrations"
- Storytelling: "The Day Gmail Launched with 1GB of Storage"
"""
```

**Implementation:**
1. Update `generate_article_title()` with banned phrase check
2. If generated title contains banned phrase, regenerate (max 2 retries)
3. Track title patterns in feedback loop

---

### 6.2: Title Template Variety (Day 3)

**Create diverse title templates:**

```python
TITLE_TEMPLATES = {
    "how_pattern": "How {Subject} {Verb} {Object}",
    "number_pattern": "{Number} Ways {Subject} {Impact}",
    "question_pattern": "Why {Question}?",
    "comparison": "{A} vs {B}: {Key Difference}",
    "case_study": "Inside {Company}'s {Innovation}",
    "technical_spec": "{Technology}: {Specific Feature} Explained",
    "historical": "The {Year} {Event} That Changed {Field}",
    "controversial": "{Common Belief} Is Wrong About {Topic}"
}
```

**Testing:**
- Generate 50 titles with new system
- Check: Zero banned verbs
- Verify: Titles feel fresh and varied
- A/B test: Compare old vs new title engagement (if metrics available)

---

## Phase 7: Data Visualization Integration (Week 12-13)
**Goal:** Add richer data visualizations and charts  
**Complexity:** High (requires external tools or generation)  
**Cost Impact:** Variable (R visualizations could be expensive)

### 7.1: Mermaid Diagram Generation (Day 1-3)

**Already partially implemented, enhance:**

```python
def generate_mermaid_diagram(
    article_context: str,
    diagram_type: str  # "flowchart", "sequence", "graph", "timeline"
) -> Optional[str]:
    """
    Generate Mermaid diagram code for article.
    
    Uses GPT-4o-mini to create appropriate diagrams based on content.
    """
    
    prompt = f"""
Create a Mermaid diagram to visualize this concept from the article:

CONTEXT:
{article_context}

DIAGRAM TYPE: {diagram_type}

Requirements:
- Clear, readable labels
- Logical flow or structure
- 5-10 nodes (not too complex)
- Use appropriate Mermaid syntax

Return ONLY the Mermaid code block, nothing else.
"""
    
    # Generate diagram
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.3
    )
    
    # Validate Mermaid syntax
    mermaid_code = extract_mermaid_code(response.choices[0].message.content)
    if validate_mermaid_syntax(mermaid_code):
        return mermaid_code
    
    return None
```

**Integration:**
- For DEEP_DIVE and EPIC articles, generate 1-2 Mermaid diagrams
- Insert at logical points in article structure
- Add to prompt: "I've prepared a diagram showing [concept], which will be inserted here"

---

### 7.2: Data Table Generation (Day 4-5)

**Enhance comparison articles with rich tables:**

```python
def generate_comparison_table(
    items_to_compare: List[str],
    criteria: List[str],
    article_context: str
) -> str:
    """
    Generate markdown comparison table.
    """
    
    prompt = f"""
Create a comparison table for these items:

ITEMS: {', '.join(items_to_compare)}
CRITERIA: {', '.join(criteria)}

CONTEXT:
{article_context}

Return a markdown table with:
- Items as columns
- Criteria as rows
- Concise, factual entries (2-5 words each)
- Include a summary row if appropriate

Example format:
| Criteria | Item A | Item B | Item C |
|----------|--------|--------|--------|
| Performance | Fast | Medium | Slow |
| Cost | $$ | $ | $$$ |
"""
    
    # Generate table
    # ... (implementation)
```

---

### 7.3: R Visualization Experiment (Day 6-8, Optional)

**ADVANCED: Generate data visualizations with R**

Only if:
- Article has actual data (not common)
- EPIC length article with deep analysis
- Budget allows ($0.10-0.20 per visualization)

**Approach:**
1. Extract data points from article
2. Generate R script to visualize
3. Execute R script in sandboxed environment
4. Convert output to PNG/SVG
5. Upload to image hosting
6. Embed in article

**File structure:**
```
src/visualizations/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ r_generator.py      # Generate R scripts
â”œâ”€â”€ executor.py         # Safe R execution
â””â”€â”€ uploader.py         # Image hosting
```

**Caution:**
- Requires R installation in pipeline environment
- Security concerns (execute arbitrary R code)
- Cost and complexity high
- Recommend: Start with manual R visualization for select articles

---

## Implementation Checklist

### Week 1-2: Voice Profiles âœ…
- [ ] Define 7 voice profiles with detailed style guides
- [ ] Implement voice selection algorithm
- [ ] Create voice rotation tracker
- [ ] Integrate into pipeline
- [ ] Generate test batch (20 articles)
- [ ] Manual review: Confirm voices feel distinct
- [ ] Deploy to production

### Week 3-4: Dynamic Length âœ…
- [ ] Create length profile system
- [ ] Implement complexity scoring
- [ ] Add length selection algorithm
- [ ] Integrate model routing (GPT-4o for deep-dives)
- [ ] Update config with budget controls
- [ ] Generate test articles at each length tier
- [ ] Validate length distribution (50% standard, 25% extended, 15% deep, 10% quick/epic)
- [ ] Deploy to production

### Week 5-6: Quality Feedback âœ…
- [ ] Implement AI reviewer (GPT-4o-mini)
- [ ] Create review criteria and scoring
- [ ] Build A/B comparison system
- [ ] Set up feedback tracking (quality_feedback.json)
- [ ] Test regeneration with feedback loop
- [ ] Run A/B test: old voice vs new voice system
- [ ] Analyze feedback patterns
- [ ] Deploy adaptive improvements

### Week 7-8: Structural Variety âœ…
- [ ] Create hook variety system (6 hook types)
- [ ] Build structure template library (8-10 templates)
- [ ] Implement structure selection logic
- [ ] Add visual element prompts (diagrams, tables, code)
- [ ] Test each structure template
- [ ] Generate batch with varied structures
- [ ] Manual review: Confirm variety
- [ ] Deploy to production

### Week 9-10: Pacing & Perspective ðŸ”„
- [ ] Add pacing control to voice profiles
- [ ] Generate articles with varied pacing
- [ ] Test perspective variations (2nd, 1st inclusive, 3rd objective)
- [ ] Measure engagement metrics (if available)
- [ ] Refine based on feedback
- [ ] Deploy conservative perspective changes

### Week 11: Title Variety & Banned Phrases âœ…
- [ ] Create banned phrase filter
- [ ] Implement title template variety
- [ ] Update title generation with filters
- [ ] Generate 50 test titles
- [ ] Verify zero banned verbs
- [ ] Deploy to production

### Week 12-13: Data Visualizations ðŸš€
- [ ] Enhance Mermaid diagram generation
- [ ] Implement comparison table generation
- [ ] Test visual elements in long-form articles
- [ ] (Optional) R visualization experiment
- [ ] Deploy enhanced visuals

---

## Cost Projections

### Current Baseline (per article)
- Content generation: $0.0008-0.0012 (GPT-4o-mini)
- Title/slug generation: $0.00005
- Image selection: $0.001 (stock photos) or $0.020 (DALL-E fallback)
- **Total: ~$0.002-0.022 per article**

### After Implementation (per article)
- Content generation: 
  - Standard (50%): $0.0010 (GPT-4o-mini)
  - Extended (25%): $0.0015 (GPT-4o-mini)
  - Deep-dive (15%): $0.015 (GPT-4o)
  - Epic (10%): $0.030 (GPT-4-turbo)
  - **Weighted average: ~$0.007**
- Voice selection: $0.00008 (GPT-4o-mini)
- Length selection: $0.00008 (GPT-4o-mini)
- Structure selection: $0.00005 (GPT-4o-mini)
- AI review: $0.0003 (GPT-4o-mini)
- Title generation: $0.00005
- Diagram generation (15% of articles): $0.0002 avg
- Image selection: $0.001 avg
- **Total: ~$0.009 per article**

**Increase: ~4-5x current cost, but:**
- Deep-dive articles (15%) provide premium content
- Quality improvements may increase engagement
- Budget controls prevent runaway costs
- Can disable deep-dive/epic if budget constrained

---

## Testing & Validation Strategy

### Per-Phase Testing
Each phase includes:
1. **Unit tests**: Core logic (voice selection, length calculation)
2. **Integration tests**: Full pipeline with new features
3. **Manual review**: Generate 10-20 test articles, human assessment
4. **A/B comparison**: Compare old vs new system (if applicable)
5. **Cost validation**: Verify cost increases are within budget

### Quality Metrics to Track
- Voice distribution (should be balanced, no >30% single voice)
- Length distribution (matches target: 50/25/15/10 split)
- Review scores (average >7.0/10 overall)
- Variety metrics:
  - Header diversity (how many unique H2 patterns?)
  - Title variety (no banned verbs, diverse structures)
  - Hook variety (even distribution across 6 hook types)
- Cost per article (should stay under $0.015 average)

### Rollout Strategy
- **Phase 1-2**: Deploy to production after validation (core features)
- **Phase 3**: Run in shadow mode for 1 week (review but don't block)
- **Phase 4-5**: Deploy incrementally, monitor metrics
- **Phase 6**: Quick deploy (low risk)
- **Phase 7**: Optional, deploy only if budget allows

---

## Configuration & Feature Flags

```python
# In src/config.py

# Voice system
VOICE_SYSTEM_ENABLED: bool = os.getenv("VOICE_SYSTEM_ENABLED", "true").lower() == "true"
MIN_VOICE_GAP: int = int(os.getenv("MIN_VOICE_GAP", "2"))  # Don't repeat voice within N articles

# Length variation
LENGTH_VARIATION_ENABLED: bool = os.getenv("LENGTH_VARIATION_ENABLED", "true").lower() == "true"
ALLOW_DEEP_DIVE: bool = os.getenv("ALLOW_DEEP_DIVE", "true").lower() == "true"
ALLOW_EPIC: bool = os.getenv("ALLOW_EPIC", "false").lower() == "true"  # Default off (expensive)
DEEP_DIVE_BUDGET_PER_RUN: float = float(os.getenv("DEEP_DIVE_BUDGET", "0.50"))

# Quality system
AI_REVIEW_ENABLED: bool = os.getenv("AI_REVIEW_ENABLED", "true").lower() == "true"
ALLOW_REGENERATION: bool = os.getenv("ALLOW_REGENERATION", "true").lower() == "true"
MIN_REVIEW_SCORE: float = float(os.getenv("MIN_REVIEW_SCORE", "6.0"))

# Structural variety
STRUCTURE_VARIETY_ENABLED: bool = os.getenv("STRUCTURE_VARIETY_ENABLED", "true").lower() == "true"
HOOK_VARIETY_ENABLED: bool = os.getenv("HOOK_VARIETY_ENABLED", "true").lower() == "true"

# Visual elements
MERMAID_DIAGRAMS_ENABLED: bool = os.getenv("MERMAID_DIAGRAMS_ENABLED", "true").lower() == "true"
COMPARISON_TABLES_ENABLED: bool = os.getenv("COMPARISON_TABLES_ENABLED", "true").lower() == "true"
R_VISUALIZATIONS_ENABLED: bool = os.getenv("R_VISUALIZATIONS_ENABLED", "false").lower() == "true"

# Model preferences
MAX_MODEL_TIER: str = os.getenv("MAX_MODEL_TIER", "standard")  # "mini" | "standard" | "turbo"
REVIEW_MODEL: str = os.getenv("REVIEW_MODEL", "gpt-4o-mini")  # Model for AI reviews
```

---

## Next Steps

1. **Review this plan** - Does this match your vision?
2. **Prioritize phases** - Which order makes most sense?
3. **Set budget** - Max $ per article? Max $ per pipeline run?
4. **Choose starting point** - Begin with Phase 1 (voices)?

Once you approve the direction, I can start implementing **Phase 1.1: Voice Profile Definitions** today! ðŸš€

---

## Decisions & Next Steps âœ…

### Decisions Made:

1. **Voice personas**: âœ… Use alliterative names WITHOUT the persona label
   - Example: "Aria" (analyst), "Taylor" (technical), "Sam" (storyteller)
   - NOT: "Alex the Analyst" âŒ

2. **Length distribution**: âœ… Start with 50/25/15/10, make adaptive later
   - Track distribution in feedback loop
   - Adjust thresholds if articles cluster too much in one tier

3. **Review threshold**: âœ… Start at 6.0, make adaptive based on observations
   - Track regeneration rate
   - If >30% regenerations: lower threshold to 5.5
   - If <5% regenerations: raise threshold to 6.5

4. **Epic articles**: âœ… Enable from start with weekly cap
   - Max 1-2 epic articles per week (~7-14 per pipeline run)
   - Use complexity score >0.95 as gate
   - Budget cap: $0.60 per run for epics ($0.30 Ã— 2 max)

5. **R visualizations**: â¸ï¸ Skip for Phase 1, revisit later
   - Implement when we have data-heavy articles
   - Could integrate with open data APIs (World Bank, FRED, etc.)
   - Phase 7 optional implementation

6. **Review model**: âœ… Adaptive based on article length
   - Quick/Standard: GPT-4o-mini (cheap, sufficient)
   - Extended: GPT-4o-mini (still good)
   - Deep-dive/Epic: GPT-4o (better quality assessment, worth the cost)
   - Reasoning: Long articles justify better review quality

---

## ðŸš€ Ready to Implement!

**Next Action:** Begin Phase 1.1 - Voice Profile Definitions

We'll create:
- 7 voice profiles with alliterative names (no labels)
- Smart voice selection with rotation
- Integration into pipeline

Should I start coding now? ðŸŽ¯
