#!/usr/bin/env python3
"""Analyze evaluation results and generate recommendation."""

import json
from pathlib import Path

from rich.console import Console

console = Console()


def analyze_evaluation(eval_dir: Path, output_file: Path):
    """Analyze all evaluation results and generate recommendation."""

    # Load all results
    comparison = json.load(open(eval_dir / "comparison-results.json"))
    ab_test = json.load(open(eval_dir / "ab-test-results.json"))
    complex_test = json.load(open(eval_dir / "complex-article-test.json"))

    # Find best models
    comparison_results = comparison["results"]
    successful = [r for r in comparison_results if r["success"]]

    best_value = min(successful, key=lambda x: x["cost_per_1k_words"])
    fastest = min(successful, key=lambda x: x["avg_elapsed_seconds"])
    most_efficient = min(
        successful, key=lambda x: x["avg_output_tokens"] / x["avg_word_count"]
    )

    # Find best AB config
    ab_configs = ab_test.get("configs", {})
    successful_configs = {k: v for k, v in ab_configs.items() if v.get("success")}
    best_config = min(successful_configs.items(), key=lambda x: x[1]["avg_total_cost"])

    # Generate recommendation markdown
    md = f"""# Quarterly Model Evaluation Results

## Summary

**Evaluation Date:** {comparison["timestamp"][:10]}
**Models Tested:** {len(comparison_results)}

## Quick Comparison Results

### Best Performers

- **ðŸ’° Best Value:** `{best_value["model"]}` (${best_value["cost_per_1k_words"]:.4f} per 1K words)
- **âš¡ Fastest:** `{fastest["model"]}` ({fastest["avg_elapsed_seconds"]:.1f}s average)
- **ðŸŽ¯ Most Token Efficient:** `{most_efficient["model"]}` ({most_efficient["avg_output_tokens"] / most_efficient["avg_word_count"]:.1f} tokens/word)

### Model Comparison Table

| Model | Words | Tokens/Word | Time (s) | Cost/1K words |
|-------|-------|-------------|----------|---------------|
"""

    for result in sorted(successful, key=lambda x: x["cost_per_1k_words"]):
        tok_per_word = result["avg_output_tokens"] / result["avg_word_count"]
        md += f"| `{result['model']}` | {result['avg_word_count']:.0f} | {tok_per_word:.1f} | {result['avg_elapsed_seconds']:.1f} | ${result['cost_per_1k_words']:.4f} |\n"

    md += f"""

## A/B Test Results

**Best Configuration:** {best_config[0]}

### Cost Analysis

| Configuration | Cost/Article | Monthly Cost (270 articles) |
|--------------|--------------|----------------------------|
"""

    for name, data in sorted(
        successful_configs.items(), key=lambda x: x[1]["avg_total_cost"]
    ):
        md += f"| {name} | ${data['avg_total_cost']:.4f} | ${data['monthly_cost_estimate_270_articles']:.2f} |\n"

    md += f"""

## Complex Article Test

Long-form (1500+ word) technical article results:

"""

    for result in complex_test["results"]:
        if result["success"]:
            md += f"""### {result["model"]}
- **Words:** {result["word_count"]}
- **Tokens/Word:** {result["tokens_per_word"]:.1f}
- **Cost:** ${result["cost"]:.4f}
- **Time:** {result["elapsed_seconds"]:.1f}s
- **Readability (Flesch):** {result["readability"]["flesch_reading_ease"]:.1f}

"""

    md += f"""## Recommendation

**Optimal Configuration:** {best_config[0]}

### Recommended Model Assignments

Based on cost/performance analysis:

```python
# src/config.py
content_model = "{best_config[1]["config"]["content_model"]}"
title_model = "{best_config[1]["config"]["title_model"]}"
enrichment_model = "{best_config[1]["config"]["enrichment_model"]}"
review_model = "{best_config[1]["config"]["review_model"]}"
```

### Cost Impact

- **Current Estimated Monthly:** ${best_config[1]["monthly_cost_estimate_270_articles"]:.2f}
- **Per Article:** ${best_config[1]["avg_total_cost"]:.4f}

### Key Findings

1. **Token Efficiency:** Best models maintain {most_efficient["avg_output_tokens"] / most_efficient["avg_word_count"]:.1f} tokens/word
2. **Performance:** Average generation time {fastest["avg_elapsed_seconds"]:.1f}s
3. **Cost Optimization:** Recommended config is ${best_config[1]["avg_total_cost"]:.4f} per article

### Action Items

- [ ] Review this PR and approve if recommendations look good
- [ ] Merge to update production config
- [ ] Monitor first week for any quality/cost issues
- [ ] Update MODEL-EVALUATION-STRATEGY.md if needed

---

*This evaluation was automatically generated by GitHub Actions. All test results are stored in `data/quarterly-eval-*/` for detailed review.*
"""

    with open(output_file, "w") as f:
        f.write(md)

    console.print(f"[green]Recommendation saved to: {output_file}[/green]")


def main():
    import argparse

    parser = argparse.ArgumentParser(description="Analyze evaluation results")
    parser.add_argument("--eval-dir", required=True, help="Evaluation directory")
    parser.add_argument("--output", required=True, help="Output markdown file")

    args = parser.parse_args()

    analyze_evaluation(Path(args.eval_dir), Path(args.output))


if __name__ == "__main__":
    main()
